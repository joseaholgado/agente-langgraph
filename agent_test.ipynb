{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d99aecc9",
      "metadata": {},
      "source": [
        "## Lectura de PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e3a22491",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path=\"Investigación de WindSurf.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = []\n",
        "async for page in loader.alazy_load():\n",
        "    pages.append(page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "495458e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'producer': 'Skia/PDF m137 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Investigación de WindSurf', 'source': 'Investigación de WindSurf.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}\n",
            "\n",
            "INFORME\n",
            " \n",
            "WINDSURF\n",
            "                      \n",
            "5.  ¿Qué  tipo  de  almacenamiento  usa  y  dónde  se  \n",
            "almacenan?\n",
            " \n",
            "Su  almacenamiento  es  de  tipo  Vectorial.  Esto  se  debe  a  que  las  incrustaciones  son  \n",
            "representaciones\n",
            " \n",
            "numéricas\n",
            " \n",
            "de\n",
            " \n",
            "alta\n",
            " \n",
            "dimensión.\n",
            "  El  lugar  donde  se  almacena  depende  de  su  versión:   En  el  escenario  Local ,  las  incrustaciones  se  guardan  directamente  en  el  dispositivo  del  \n",
            "usuario\n",
            " \n",
            "bajo\n",
            " \n",
            "cifrado\n",
            " \n",
            "AES-256,\n",
            " \n",
            "mientras\n",
            " \n",
            "que\n",
            " \n",
            "los\n",
            " \n",
            "índices\n",
            " \n",
            "están\n",
            " \n",
            "optimizados\n",
            " \n",
            "para\n",
            " \n",
            "ejecutarse\n",
            " \n",
            "localmente\n",
            " \n",
            "en\n",
            " \n",
            "CPU\n",
            " \n",
            "o\n",
            " \n",
            "GPU.\n",
            " \n",
            "En\n",
            " \n",
            "el\n",
            " \n",
            "escenario\n",
            " \n",
            "Enterprise\n",
            ",\n",
            " \n",
            "la\n",
            " \n",
            "información\n",
            " \n",
            "se\n",
            " \n",
            "administra\n",
            " \n",
            "en\n",
            " \n",
            "servidores\n",
            " \n",
            "aislados,\n",
            " \n",
            "lo\n",
            " \n",
            "que\n",
            " \n",
            "aporta\n",
            " \n",
            "una\n",
            " \n",
            "capa\n",
            " \n",
            "extra\n",
            " \n",
            "de\n",
            " \n",
            "seguridad\n",
            " \n",
            "y\n",
            " \n",
            "escalabilidad.\n",
            " \n",
            "Además,\n",
            " \n",
            "en\n",
            " \n",
            "ambos\n",
            " \n",
            "casos,\n",
            " \n",
            "el\n",
            " \n",
            "código\n",
            " \n",
            "original\n",
            " \n",
            "no\n",
            " \n",
            "persiste\n",
            " \n",
            "en\n",
            " \n",
            "el\n",
            " \n",
            "almacenamiento\n",
            " \n",
            "a\n",
            " \n",
            "largo\n",
            " \n",
            "plazo:\n",
            " \n",
            "solo\n",
            " \n",
            "se\n",
            " \n",
            "emplea\n",
            " \n",
            "para\n",
            " \n",
            "la\n",
            " \n",
            "generación\n",
            " \n",
            "de\n",
            " \n",
            "los\n",
            " \n",
            "vectores\n",
            " \n",
            "y,\n",
            " \n",
            "al\n",
            " \n",
            "concluir\n",
            " \n",
            "dicho\n",
            " \n",
            "proceso,\n",
            " \n",
            "se\n",
            " \n",
            "descarta\n",
            " \n",
            "para\n",
            " \n",
            "mantener\n",
            " \n",
            "la\n",
            " \n",
            "confidencialidad\n",
            " \n",
            "del\n",
            " \n",
            "proyecto\n",
            "  Se  muestra  una  tabla  para  mejor  visualización:   Escenario  Local  (Ej:  Dispositivo  del  Usuario)  \n",
            "Enterprise  (Servidores  de  Windsurf)  \n",
            "Incrustaciones  En  disco,  cifradas  (AES-256).  En  servidores  single-tenant  aislados  y  cifrados.  \n",
            "Índices  Optimizados  para  hardware  local  (CPU/GPU).  \n",
            "Alojados  en  infraestructura  escalable  (en  la  nube).  \n",
            "Código  original  No  se  almacena  (solo  embeddings).  \n",
            "No  se  almacena  (se  borra  tras  la  indexación).   \n",
            "6.  ¿Cómo  se  almacenan  las  incrustaciones  en  bases  de  \n",
            "datos\n",
            " \n",
            "vectoriales?\n",
            " \n",
            "El  motor  de  Indexación  funciona  en  parte  generando  incrustaciones  para  su  base  de  código  \n",
            "que\n",
            " \n",
            "capturan\n",
            " \n",
            "el\n",
            " \n",
            "significado\n",
            " \n",
            "subyacente.\n",
            " \n",
            "Estas\n",
            " \n",
            "incrustaciones\n",
            " \n",
            "pueden\n",
            " \n",
            "consultarse\n",
            " \n",
            "utilizando\n",
            " \n",
            "tanto\n",
            " \n",
            "el\n",
            " \n",
            "Lenguaje\n",
            " \n",
            "Natural\n",
            " \n",
            "como\n",
            " \n",
            "los\n",
            " \n",
            "fragmentos\n",
            " \n",
            "de\n",
            " \n",
            "código\n",
            " \n",
            "relacionados.\n",
            "  \n",
            "índices\n",
            " \n",
            "especializados\n",
            " \n",
            "como\n",
            " \n",
            "HNSW\n",
            " \n",
            "o\n",
            " \n",
            "IVF\n",
            " \n",
            "diseñados\n",
            " \n",
            "para\n",
            " \n",
            "agilizar\n",
            " \n",
            "la\n",
            " \n",
            "búsqueda\n",
            " \n",
            "de\n",
            " \n",
            "similitud.\n",
            " \n",
            "El  sistema  combina  los  embeddings  generados  con  un  framework  RAG,  habilitando  al  \n",
            "modelo\n",
            " \n",
            "de\n",
            " \n",
            "lenguaje\n",
            " \n",
            "para\n",
            " \n",
            "acceder\n",
            " \n",
            "de\n",
            " \n",
            "inmediato\n",
            " \n",
            "a\n",
            " \n",
            "fragmentos\n",
            " \n",
            "contextualmente\n",
            " \n",
            "relevantes.\n",
            " \n",
            "Esta\n",
            " \n",
            "integración\n",
            " \n",
            "mejora\n",
            " \n",
            "la\n",
            " \n",
            "exactitud\n",
            " \n",
            "de\n",
            " \n",
            "las\n",
            " \n",
            "respuestas\n",
            " \n",
            "y\n",
            " \n",
            "reduce\n",
            " \n",
            "sustancialmente\n",
            " \n",
            "las\n",
            " \n",
            "alucinaciones,\n",
            " \n",
            "ya\n",
            " \n",
            "que\n",
            " \n",
            "el\n",
            " \n",
            "modelo\n",
            " \n",
            "recibe\n",
            " \n",
            "respaldo\n",
            " \n",
            "contextual\n",
            " \n",
            "antes\n",
            " \n",
            "de\n",
            " \n",
            "producir\n",
            " \n",
            "cualquier\n",
            " \n",
            "resultado\n",
            " \n",
            "final\n",
            " \n",
            " \n",
            "6\n"
          ]
        }
      ],
      "source": [
        "print(f\"{pages[5].metadata}\\n\")\n",
        "print(pages[5].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63a91461",
      "metadata": {},
      "source": [
        "## Búsqueda vectorial sobre pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "47cf1ed9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"API_KEY\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f8fba335",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Page 3: INFORME\n",
            " \n",
            "WINDSURF\n",
            "                      \n",
            "puede  ofrecer  respuestas  y  soluciones  más  precisas  y  relevantes,  adaptándose  al  flujo  de  \n",
            "trabajo\n",
            " \n",
            "del\n",
            " \n",
            "usuario\n",
            " \n",
            "de\n",
            " \n",
            "manera\n",
            " \n",
            "eficiente.\n",
            "  Análisis :  Una  vez  que  Cascade  recibe  su  entrada,  analiza  tanto  el  código  seleccionado  \n",
            "c\n",
            "\n",
            "Page 2: INFORME\n",
            " \n",
            "WINDSURF\n",
            "                      \n",
            "1.  Características  claves  \n",
            "Es  similar  a  Cursor,  pero  con  una  peculiaridad:  utiliza  Flows,  que  combinan  Agentes  y  \n",
            "Copilots.\n",
            " \n",
            "Al\n",
            " \n",
            "integrar\n",
            " \n",
            "ambas\n",
            " \n",
            "herramientas,\n",
            " \n",
            "se\n",
            " \n",
            "mejora\n",
            " \n",
            "el\n",
            " \n",
            "flujo\n",
            " \n",
            "de\n",
            " \n",
            "desarrollo\n",
            " \n",
            "del\n",
            " \n",
            "programador.\n",
            " \n",
            "Con  esta\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings(api_key=api_key))\n",
        "docs = vector_store.similarity_search(\"Integra Linters?\", k=2)\n",
        "for doc in docs:\n",
        "    print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37510e25",
      "metadata": {},
      "source": [
        "# Incrustación de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0b31e6f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0f2a0089",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5, 3072)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        \"Hi there!\",\n",
        "        \"Oh, hello!\",\n",
        "        \"What's your name?\",\n",
        "        \"My friends call me World\",\n",
        "        \"Hello World!\"\n",
        "    ]\n",
        ")\n",
        "len(embeddings), len(embeddings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b235d44",
      "metadata": {},
      "source": [
        "## Lector de PDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bc2583",
      "metadata": {},
      "source": [
        "### Carga y extracción de texto desde PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "45104921",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "# Cargar múltiples PDFs\n",
        "pdf_paths = [\"Investigación de WindSurf.pdf\", \"nke-10k-2023.pdf\"]\n",
        "docs = []\n",
        "for path in pdf_paths:\n",
        "    loader = PyMuPDFLoader(path)\n",
        "    pages = loader.load()  # pages es lista de Document con page_content y metadata\n",
        "    docs.extend(pages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91076089",
      "metadata": {},
      "source": [
        "### División de texto en fragmentos (chunking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "749184ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, chunk_overlap=150)\n",
        "chunks = splitter.split_documents(docs)  # Se generan Document separados en trozos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7127e2c",
      "metadata": {},
      "source": [
        "### Generación de embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "866e3a11",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"TU_API_KEY\"  # Configura tu API key de OpenAI\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b236c69",
      "metadata": {},
      "source": [
        "### Almacenar embeddings en FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "e8c787f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO: Loading faiss with AVX2 support.\n",
            "INFO: Successfully loaded faiss with AVX2 support.\n",
            "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(chunks, embeddings_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e671295",
      "metadata": {},
      "source": [
        "###  Responder consultas vía similitud semántica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "792a6c71",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO: HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Cascade es una herramienta de inteligencia artificial que ofrece respuestas y soluciones precisas y relevantes para el flujo de trabajo del usuario. Utiliza reglas y memorias configuradas, selecciona un modelo de IA adecuado, accede a diversas herramientas y genera sugerencias de código, modificaciones o respuestas en función de la solicitud del usuario. También está integrado con linters y puede identificar problemas en el código y ofrecer soluciones.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "retriever = db.as_retriever()  # transforma FAISS en un retriever\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(api_key=api_key,temperature=0),        # modelo de lenguaje (e.g. GPT-3.5)\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "query = \"¿Que es Cascade?\"\n",
        "result = qa_chain.run(query)\n",
        "print(result)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
